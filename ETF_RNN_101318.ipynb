{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQQ\n",
      "SPY\n",
      "DIA\n",
      "IWM\n",
      "0.02208672341597145\n",
      "            QQQ_close    future  target\n",
      "date                                   \n",
      "05/26/2000   77.62480  92.94000       1\n",
      "05/30/2000   85.00000  92.75000       1\n",
      "05/31/2000   83.09375  93.68750       1\n",
      "06/01/2000   87.62500  90.86135       1\n",
      "06/02/2000   93.50050  94.06600       1\n",
      "06/05/2000   93.86000  91.93440       0\n",
      "06/06/2000   91.18680  93.65710       1\n",
      "06/07/2000   92.94000  94.43800       1\n",
      "06/08/2000   92.75000  98.05319       1\n",
      "06/09/2000   93.68750  97.93800       1\n",
      "            QQQ_close  QQQ_volume   QQQ_21ma    QQQ_7ma  QQQ_ratio_ma  \\\n",
      "date                                                                    \n",
      "05/26/2000   77.62480    18086776  85.820114  79.896586      1.074140   \n",
      "05/30/2000   85.00000    30061564  85.367690  79.798543      1.069790   \n",
      "05/31/2000   83.09375    29029600  84.772455  80.013364      1.059479   \n",
      "06/01/2000   87.62500    28897100  84.644121  80.870507      1.046662   \n",
      "06/02/2000   93.50050    32000192  84.856955  83.406293      1.017393   \n",
      "\n",
      "            SPY_close  SPY_volume    SPY_21ma     SPY_7ma  SPY_ratio_ma  \\\n",
      "date                                                                      \n",
      "05/26/2000  138.31250     4806700  142.263393  140.071429      1.015649   \n",
      "05/30/2000  142.62500     5200200  142.139881  139.866071      1.016257   \n",
      "05/31/2000  142.40625     6046100  141.912202  140.084821      1.013045   \n",
      "06/01/2000  145.15625     8420500  141.937500  140.763393      1.008341   \n",
      "06/02/2000  148.09375     8673100  142.245536  142.267857      0.999843   \n",
      "\n",
      "             ...      DIA_21ma     DIA_7ma  DIA_ratio_ma  IWM_close  \\\n",
      "date         ...                                                      \n",
      "05/26/2000   ...    105.930060  105.031250      1.008558      45.72   \n",
      "05/30/2000   ...    105.837798  104.691964      1.010945      47.41   \n",
      "05/31/2000   ...    105.690476  104.584821      1.010572      47.58   \n",
      "06/01/2000   ...    105.657738  104.745536      1.008709      48.66   \n",
      "06/02/2000   ...    105.785714  105.263393      1.004962      51.16   \n",
      "\n",
      "            IWM_volume   IWM_21ma    IWM_7ma  IWM_ratio_ma    future  target  \n",
      "date                                                                          \n",
      "05/26/2000     76800.0  45.720000  45.720000           1.0  92.94000       1  \n",
      "05/30/2000     57600.0  46.565000  46.565000           1.0  92.75000       1  \n",
      "05/31/2000     36000.0  46.903333  46.903333           1.0  93.68750       1  \n",
      "06/01/2000      7000.0  47.342500  47.342500           1.0  90.86135       1  \n",
      "06/02/2000     29388.0  48.106000  48.106000           1.0  94.06600       1  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "train data: 4365 validation: 202\n",
      "Dont buys: 1900, buys: 2465\n",
      "VALIDATION Dont buys: 77, buys: 125\n",
      "202\n",
      "Train on 4365 samples, validate on 202 samples\n",
      "Epoch 1/100\n",
      "4365/4365 [==============================] - 28s 6ms/step - loss: 0.8052 - acc: 0.5414 - val_loss: 0.6944 - val_acc: 0.4505\n",
      "Epoch 2/100\n",
      "4365/4365 [==============================] - 14s 3ms/step - loss: 0.7278 - acc: 0.5766 - val_loss: 0.6998 - val_acc: 0.4109\n",
      "Epoch 3/100\n",
      "4365/4365 [==============================] - 15s 3ms/step - loss: 0.7039 - acc: 0.5853 - val_loss: 0.7023 - val_acc: 0.3713\n",
      "Epoch 4/100\n",
      "3072/4365 [====================>.........] - ETA: 4s - loss: 0.6856 - acc: 0.5859"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-daf796921835>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    181\u001b[0m \u001b[1;31m# Score model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    212\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2977\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2978\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2979\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2980\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import random\n",
    "from numpy.random import shuffle\n",
    "from sklearn import preprocessing \n",
    "from collections import deque\n",
    "# from tensorflow.contrib.rnn import *\n",
    "\n",
    "# df = pd.read_table(\"E:/Stock Data/30 min data/QQQ.txt\", delimiter=',', \n",
    "#                    names=['date', 'time', 'low', 'high', 'open', 'close', 'volume'])\n",
    "\n",
    "SEQ_LEN = 30  # how long of a preceeding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 7  # how far into the future are we trying to predict?\n",
    "RATIO_TO_PREDICT = \"QQQ\"\n",
    "\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df = df.drop(\"future\", 1)  # don't need this anymore.\n",
    "\n",
    "#     for col in df.columns:  # go through all of the columns\n",
    "#         if col != \"target\":  # normalize all ... except for the target itself!\n",
    "#             df[col] = df[col].pct_change()  # pct change \"normalizes\" \n",
    "#             df.dropna(inplace=True)  # remove the nas created by pct_change\n",
    "#             df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
    "    \n",
    "    df.dropna(inplace=True)  # cleanup\n",
    "    \n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  #They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    " \n",
    "    for i in df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # check for 60 sequences\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  \n",
    "            \n",
    "    np.random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:  # going over our new sequential data\n",
    "        X.append(seq)  # X is the sequences\n",
    "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
    "  \n",
    "    return np.array(X), y  # return X and y...and make X a numpy array\n",
    "\n",
    "main_df = pd.DataFrame() # begin empty\n",
    "\n",
    "ratios = [\"QQQ\", \"SPY\", \"DIA\", \"IWM\"]  # the 4 ETFs we want to consider\n",
    "for ratio in ratios:  # begin iteration\n",
    "    print(ratio)\n",
    "    dataset = f'E:/Stock Data/{ratio}.txt'  # get the full path to the file.\n",
    "    df = pd.read_csv(dataset, names=['date', 'low', 'high', 'open', 'close', 'volume'])  # read in specific file\n",
    "    df['21ma'] = df['close'].rolling(window=21, min_periods=0).mean()\n",
    "    df['7ma'] = df['close'].rolling(window=7, min_periods=0).mean()\n",
    "    df['ratio_ma'] = df['21ma'] / df['7ma']\n",
    "    m,_  = np.polyfit(df.index, df['21ma'], deg=1) # is there a way to create a rolling slope based\n",
    "    #  rename volume and close to include the ETF name:\n",
    "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\", \"21ma\": f\"{ratio}_21ma\", \"7ma\": f\"{ratio}_7ma\", \"ratio_ma\": f\"{ratio}_ratio_ma\"}, inplace=True)\n",
    "\n",
    "    df.set_index(\"date\", inplace=True)  # set time as index \n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\", f\"{ratio}_21ma\", f\"{ratio}_7ma\", f\"{ratio}_ratio_ma\"]]  # ignore the other columns\n",
    "\n",
    "    if len(main_df)==0:  # if the dataframe is empty\n",
    "        main_df = df  # then it's just the current df\n",
    "    else:  # otherwise, join this data to the main one\n",
    "        main_df = main_df.join(df)\n",
    "\n",
    "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
    "main_df.dropna(inplace=True)\n",
    "\n",
    "# print(main_df.head())  \n",
    "\n",
    "print(m21)\n",
    "    \n",
    "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
    "\n",
    "print(main_df[[f'{RATIO_TO_PREDICT}_close', 'future', 'target']].head(10))\n",
    "# print(main_df.future)\n",
    "print(main_df.head())\n",
    "sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    "\n",
    "times = sorted(main_df.index.values)  # get the times\n",
    "last_5pct = sorted(main_df.index.values)[-int(0.05*len(times))]  # get the last 5% of the times\n",
    "\n",
    "validation_main_df = main_df[(main_df.index >= last_5pct)]  # make the validation data where the index is in the last 5%\n",
    "main_df = main_df[(main_df.index < last_5pct)]  # now the main_df is all the data up to the last 5%\n",
    "\n",
    "buys = []  # list that will store our buy sequences and targets\n",
    "sells = []  # list that will store our sell sequences and targets\n",
    "\n",
    "for seq, target in sequential_data:  # iterate over the sequential data\n",
    "    if target == 0:  # if it's a \"not buy\"\n",
    "        sells.append([seq, target])  # append to sells list\n",
    "    elif target == 1:  # otherwise if the target is a 1...\n",
    "        buys.append([seq, target])  # it's a buy!\n",
    "\n",
    "np.random.shuffle(buys)  # shuffle the buys\n",
    "np.random.shuffle(sells)  # shuffle the sells!\n",
    "\n",
    "lower = min(len(buys), len(sells))  # what's the shorter length?\n",
    "\n",
    "buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
    "sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
    "\n",
    "sequential_data = buys+sells  # add them together\n",
    "np.random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other\n",
    "\n",
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)\n",
    "\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")\n",
    "\n",
    "print (len(validation_y))\n",
    "import time\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 512\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"  # a unique name for the model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# model.add(LSTM(256, return_sequences=True))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# TensorBoard callback:\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best one\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint],\n",
    "    )\n",
    "# Score model\n",
    "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# Save model\n",
    "model.save(\"models/{}\".format(NAME))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
